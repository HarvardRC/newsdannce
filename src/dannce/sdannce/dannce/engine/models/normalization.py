import torch
import torch.nn as nn


class LayerNormalization(nn.Module):
    def __init__(self, normal_shape, gamma=True, beta=True, epsilon=1e-3):
        """Layer normalization layer
        See: [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)
        :param normal_shape: The shape of the input tensor or the last dimension of the input tensor.
        :param gamma: Add a scale parameter if it is True.
        :param beta: Add an offset parameter if it is True.
        :param epsilon: Epsilon for calculating variance.
        """
        super(LayerNormalization, self).__init__()
        if isinstance(normal_shape, int):
            normal_shape = (normal_shape,)

        self.normal_shape = torch.Size(normal_shape)
        self.epsilon = epsilon
        if gamma:
            self.gamma = nn.Parameter(torch.Tensor(1))
        else:
            self.register_parameter("gamma", None)
        if beta:
            self.beta = nn.Parameter(torch.Tensor(1))
        else:
            self.register_parameter("beta", None)
        self.reset_parameters()

    def reset_parameters(self):
        if self.gamma is not None:
            self.gamma.data.fill_(1)
        if self.beta is not None:
            self.beta.data.zero_()

    def forward(self, x):
        reduction_axes = list((-1) * torch.arange(1, len(self.normal_shape) + 1))

        mean = torch.mean(x, reduction_axes, keepdims=True)
        stddev = (
            torch.mean((x - mean) ** 2, dim=reduction_axes, keepdim=True).sqrt()
            + self.epsilon
        )
        y = (x - mean) / stddev

        if self.gamma is not None:
            y *= self.gamma
        if self.beta is not None:
            y += self.beta
        return y

    def extra_repr(self):
        return "normal_shape={}, gamma={}, beta={}, epsilon={}".format(
            self.normal_shape,
            self.gamma is not None,
            self.beta is not None,
            self.epsilon,
        )
